{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_2_GNN-graphsage-no_edge_attr.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMyTEtxy9g3kmJ+7+VZmf6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scigeek72/GNN_Repo/blob/main/practice_2_GNN_graphsage_no_edge_attr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not my implementation. I am merely copying it from another notebook to get some practice about how GNNs are implemented."
      ],
      "metadata": {
        "id": "ze9dYmANImEg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o__tVTs9ITF7",
        "outputId": "3aa02cd4-2c3a-4c4b-9262-879ee306ae06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/wh1/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl size=3567071 sha256=ec67ef0778326b93f6065adf63f63e1328262874b00c6fda6848d68ede9f7e3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/57/a3/42ea193b77378ce634eb9454c9bc1e3163f3b482a35cdee4d1\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/wh1/torch-1.11.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.14.tar.gz (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 236 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.14-cp37-cp37m-linux_x86_64.whl size=1703243 sha256=0e11b0fbdc403d93224d4b7162141c5a16da5df65fa811663a2f332cb7133bf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/aa/62/db0259eae2abce84f1ee2cf1c531bba683aab4bf79054172f8\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=db54e82dcbefaf0fadec6dc504196c35dddba65b3c7bf771843a5ea4adf1c126\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.12.0+cu113)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=535c535636436c8aa554d12a6e77cb8749b228440f0dbd9176e1131fbf2b1ff6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "!pip install torch-scatter -f https://data.pyg.org/wh1/torch-1.11.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/wh1/torch-1.11.0+cu113.html\n",
        "!pip install torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch_geometric\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from tqdm import trange"
      ],
      "metadata": {
        "id": "3AM938GZMuBW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "aFssvGj4NM_T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXZTJu5QMmZP",
        "outputId": "4a726de3-18ff-41c1-80f4-f7eeb16f61fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the OGB drug-drug interaction (DDI) dataset"
      ],
      "metadata": {
        "id": "PXO_nJ5WNZAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import name\n",
        "from ogb.linkproppred import PygLinkPropPredDataset\n",
        "\n",
        "dataset_name = 'ogbl-ddi'\n",
        "\n",
        "dataset = PygLinkPropPredDataset(name= dataset_name)\n",
        "\n",
        "print(f'The {dataset_name} dataset has {len(dataset)} graph(s)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtOecOoZNWL1",
        "outputId": "e5a7c2d5-668d-44ea-d388-31c68fbec9d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/linkproppred/ddi.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.04 GB: 100%|██████████| 46/46 [00:00<00:00, 59.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/ddi.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 36.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2832.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n",
            "The ogbl-ddi dataset has 1 graph(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get more info about the dataset, look at https://ogb.stanford.edu/docs/linkprop/#ogbl-ddi "
      ],
      "metadata": {
        "id": "ZIOgxbylP91G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A graph is represented as $G = (V, E)$, where $V$ is the set of nodes, and $v \\in V$ is an individual node. $E$ is the set of edges. In torch-geometric, $E$ is represented as `edge_index` in the `COO` format (which has a shape [2, |E|] ). \n",
        "\n",
        "The first row of `edge_index` contains all the `src_nodes` and the second row contains the corresponding `dest_nodes`. \n",
        "\n",
        "** IN THE CASE OF UNDIRECTED GRAPHS, `edge_index` has shape [2, 2$×$|E|] **\n",
        "\n"
      ],
      "metadata": {
        "id": "HdIsIKj7QsF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ddi_graph = dataset[0]"
      ],
      "metadata": {
        "id": "DLvTfONFQFZR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'DDI graph object: {ddi_graph}')\n",
        "print(f'Number of nodes, |V|: {ddi_graph.num_nodes}')\n",
        "print(f'Number of (training) edges |E|: {ddi_graph.num_edges}')\n",
        "print(f'Is undirected? {ddi_graph.is_undirected()}')\n",
        "print(f'Average node degree: {ddi_graph.num_edges/ddi_graph.num_nodes:.2f}')\n",
        "print(f'Number of Node Features: {ddi_graph.num_node_features}')\n",
        "print(f'Number of Edge Features: {ddi_graph.num_edge_features}')\n",
        "print(f'Has isolated Nodes: {ddi_graph.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {ddi_graph.has_self_loops()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gslQjvEwQVIf",
        "outputId": "da55b1b5-bf3a-4d2d-c3e1-51b34790145f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DDI graph object: Data(num_nodes=4267, edge_index=[2, 2135822])\n",
            "Number of nodes, |V|: 4267\n",
            "Number of (training) edges |E|: 2135822\n",
            "Is undirected? True\n",
            "Average node degree: 500.54\n",
            "Number of Node Features: 0\n",
            "Number of Edge Features: 0\n",
            "Has isolated Nodes: False\n",
            "Has self-loops: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to explore this graph(s) with more graph related metrics, such as if it is connected? If no, how many components does it have? What are the average shortest-path lengths for each component? What's the diameter of each compnenet? etc, convert the `ddi_graph` to a `networkx` graph. `networkx` has richer set of functions that lets us do these. torch_geometric has a utility `(to_networkx)` to directly convert it to a `networkx` graph.  "
      ],
      "metadata": {
        "id": "0acN_7q4UA8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `graphsage` algorithm, we need `positive edges` and sample `negative edges` (to compute the loss function, look at the paper [Inductive representation learning on graphs](https://arxiv.org/abs/1706.02216?context=cs) by Leskovec et.al). \n",
        "\n",
        " - **Positive Edges** are the edges that exist in the graph. That is, the set { ($u$,$v$) $\\in$ $E$, where $u$ and $v$ $\\in$ $V$ }\n",
        " - **Negative Edges** edges don't exist in the graph, that is, { ($u$,$v$) $\\notin$ $E$ where $u$ and $v$ $\\in$ $V$ } \n",
        "  \n",
        "  - Note: I need to see if that is the definition provided by the authors in the above paper\n",
        "  - torch-geometric has a utility `NegativeSampler` which can be used to get **negative edges**\n",
        "\n",
        "**Note: For this excercise, we don't need to use `NegativeSample` as torch_geometric has already provided these for us. But for creating my own dataset, we need it."
      ],
      "metadata": {
        "id": "VjFnmHmVjEPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_edges = dataset.get_edge_split()\n",
        "train_edges, valid_edges, test_edges = split_edges['train'], split_edges['valid'], split_edges['test']"
      ],
      "metadata": {
        "id": "YInoo3ksTmul"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training positive edges: {len(train_edges[\"edge\"])}')\n",
        "print(f'Number of validation positive edges: {len(valid_edges[\"edge\"])}')\n",
        "print(f'Number of test positive edges: {len(test_edges[\"edge\"])}')\n",
        "print(f\"====================================\")\n",
        "print(f'Number of validation negative edges: {len(valid_edges[\"edge_neg\"])}')\n",
        "print(f'Number of test negative edges: {len(test_edges[\"edge_neg\"])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMlXq4yfl-MQ",
        "outputId": "0748eb18-33b9-4720-86d2-0f3051029b91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training positive edges: 1067911\n",
            "Number of validation positive edges: 133489\n",
            "Number of test positive edges: 133489\n",
            "====================================\n",
            "Number of validation negative edges: 101882\n",
            "Number of test negative edges: 95599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE** Although PyG has provided the negative edges for validation and test set, while training we also need to sample negative edges (to compute the loss function)."
      ],
      "metadata": {
        "id": "35jfh2_joBWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graphical Neural Network (GNN)\n",
        "\n",
        "**GNN** relies on the concept of *message passing*. For each node $v$, 'information' from it's neighboring nodes (denoted by $\\mathcal{N}(v) = \\{u, \\text{such that } (v,u) \\in E\\}$) are passed to the node $v$ as messages where these messages are aggregated (using 'a permutation-invariant function'). After aggregation, the 'representation' of the node $v$, usually denoted by $h_v$, is updated.   \n",
        "\n",
        "For implementation, we should keep in mind the following. Each layer of a GNN has 3 components.\n",
        "\n",
        " - **Message passing** between a node and it's neighbor\n",
        " - **Aggregation** of the received messages at each node\n",
        " - **Updating** of a node's representation after **aggregation**\n",
        "\n",
        " \n",
        "For the $l$-th layer, let $h_v^{l}$ be the vector representation of the node $v$. For the $(l+1)$-th layer,    each message between $v$ and $u$, where $u \\in \\mathcal{N}(v)$, $m_{vu}$ is aggregated (denoted by AGGR) at node $v$, and then $h_v^{l}$ is updated (denoted by UPDATE) to get the representation for the next layer ($l+1$) and so on. \n",
        "\n",
        "In mathematical language, the above procedure is described as follows:\n",
        "\n",
        "$$h_v^{l+1} = UPDATE(h_v^l, AGGR(\\{m_{vu}^{l+1}, ∀u \\in \\mathcal{N}(v)\\}))$$\n",
        "\n",
        "$m_{vu}$ are the messages, for a layer, from the neighbors. Many different GNN models employes different message skims. \n",
        "\n",
        "For the GraphSAGE model that we are going to use, the messages are given by $$m_{vu}^{l+1} = h_u^l, \\forall u \\in \\mathcal{N}(v)$$ It takes into consideration only the node representation of the neighboring nodes. \n",
        "\n",
        "AGGR is simply the function `mean` (note, this is a permutation-invariant function). UPDATE is simply adding these aggregated messages to $h_v^l$. \n",
        "\n",
        "Mathematically, $$h_v^{l+1} = W_1 . h_v^l + W_2 . mean(\\{m_{vu}, \\forall u \\in \\mathcal{N}(v)\\})$$\n",
        "\n",
        "Note that $W_1,W_2$ are learnable weights. \n",
        "\n",
        "Note that, the sizes of matrices $W_1$ and $W_2$ are such that after each multiplication, the **output dimensions must match the dimension of the output vector for that layer**. In this particular implementation, since all layers have same dimensions, the products should be equal to hidden_channels. \n",
        "\n",
        "For implementation, these can be achieved by a traditional neural net layers (torch.nn.Linear layer).\n",
        "\n",
        "Note: A crucial difference between our case and this case is that each node is assumed to have node features and no edge features. \n",
        "\n"
      ],
      "metadata": {
        "id": "RdB9Mr_jpb-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSAGE(torch.nn.Module):\n",
        "\n",
        "  def __init__(self,conv,in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "    super(GraphSAGE,self).__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList() ## just like list, to keep the layers\n",
        "    # add a convolutional layer. Out dimension of one layer must be the input dimension of the next layer\n",
        "    # Apply $L2$ normalization to the node representation after each layer\n",
        "    # conv is a place-holder for different convolutioanal models. For now, we will use PyG's SAGEConv implementation\n",
        "    # but later we will implement our own, custom, convolutional layers\n",
        "    # For the first layer, input has shape (|V|, in_channels), where in_channels = dim of the node features\n",
        "    self.convs.append(conv(in_channels,hidden_channels,normalize=True))\n",
        "    # For layer k=1 to (K-2), input shape:(|V|,hidden_channels), output shape: (|V|,hidden_channels)\n",
        "    for l in range(num_layers-2):\n",
        "      self.convs.append(conv(hidden_channels,hidden_channels,normalize = True))\n",
        "    # For the last layer, i,e (K-1)th layer, input_shape = hidden_channels, output_shape = out_channels\n",
        "    self.convs.append(conv(hidden_channels, out_channels, normalize=True))\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = dropout\n",
        "\n",
        "  \n",
        "  def forward(self, x, edge_index):\n",
        "    # x is the initial node features \n",
        "    for i in range(self.num_layers-1):\n",
        "      # conducts message passing, aggregation and updates for layer i\n",
        "      # x hold the same info as $h_v^l$ in the above explanation\n",
        "      x = self.convs[i](x, edge_index)\n",
        "      # when i = 0, x has shape [|V|, in_channels]\n",
        "      # for i>0, x has shape [|V|, hidden_channels]\n",
        "      # Pass through a non-linearity\n",
        "      x = F.relu(x)\n",
        "      # dropout is applied to the weight matrices (W_1, and W_2) in the above equation\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training) # not sure about this line of code\n",
        "    \n",
        "    # Final layer, generate node embeddings \n",
        "    x = self.convs[self.num_layers-1](x, edge_index)\n",
        "\n",
        "    return x \n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "hJHCz4C9mwG8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, as mentioned above, drugs, i,e nodes in this case, do not have features. To amend this, they randomly initialized node features using `torch.nn.Embedding` layer."
      ],
      "metadata": {
        "id": "UwVdSGdy_HPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters for the above model\n",
        "graphsage_in_channels = 256\n",
        "graphsage_hidden_channels = 128\n",
        "graphsage_out_channels = 256\n",
        "graphsage_num_layers = 2\n",
        "dropout = 0.5 "
      ],
      "metadata": {
        "id": "O1FrDja3_mvc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_node_embeddings = torch.nn.Embedding(ddi_graph.num_nodes, graphsage_in_channels).to(device)"
      ],
      "metadata": {
        "id": "YqtwG_xXm5Y4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our model\n",
        "graphsage_model = GraphSAGE(\n",
        "    SAGEConv, graphsage_in_channels,\n",
        "    graphsage_hidden_channels,\n",
        "    graphsage_out_channels,\n",
        "    graphsage_num_layers,\n",
        "    dropout\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "0OaRLoCcAS_B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, the model will produce, (after training which we haven't described as yet), the final embeddings of each node. The final tensor will have size (|V|, graphsage_out_channels). \n",
        "\n",
        "## DownStrem task (Link Prediction)\n",
        "\n",
        "Now we can take these embeddings and can use them in some downstream tasks. For this present work, it is link predictions, i,e, the task is to predict, given two new drugs, the probability that there will be interaction between the two drugs if taken together? To perform this, we need to define a link predictor. Below we will do that before going to describe training procedure.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2hsh9lhBGEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#link predictor parameters\n",
        "link_predictor_in_channels = graphsage_out_channels\n",
        "link_predictor_hidden_channels = link_predictor_in_channels\n",
        "\n",
        "# it has binary output (wheather link exists or not)"
      ],
      "metadata": {
        "id": "-m8QLT-1CO-n"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will describe the LinkPredictor class, which will take into account the hadamard (elementwise) product of node representations of nodes $u$ and $v$, that is, $h_v^K$ and $h_u^K$ as $h_v^K ∘ h_u^K$, and pass it through a traditional deep neural network. "
      ],
      "metadata": {
        "id": "O4rj7neQEDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinkPredictor(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, dropout, out_channels=1, el_prod=lambda x,y: x*y):\n",
        "    super(LinkPredictor, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(in_channels, hidden_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(hidden_channels, out_channels),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.el_prod = el_prod\n",
        "\n",
        "  def forward(self, u, v):\n",
        "    x = self.el_prod(u,v)\n",
        "    return self.model(x)\n"
      ],
      "metadata": {
        "id": "-kLMNf7CClO6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link_predictor = LinkPredictor(link_predictor_in_channels,link_predictor_hidden_channels,dropout).to(device)"
      ],
      "metadata": {
        "id": "OxrCwbl5dP2c"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have built our models (GNN + LinkPredictor), we need to train these models. Below we will write a training method that will train the models. \n",
        "\n",
        "Since the downstream link prediction task is a binary output task, we will use `binary crossengropy` for our loss function. However, if the downstream task is different, we need to use the appropriate loss function.\n"
      ],
      "metadata": {
        "id": "IkdXdV2d7LzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import ItemsView\n",
        "def train(graphsage_model, link_predictor, initial_node_embeddings,edge_index, pos_train_edges,\n",
        "          optimizer,batch_size):\n",
        "  total_loss, total_examples = 0,0 # in our case, we only output the loss as in other cases shown\n",
        "\n",
        "  # set our model to train (this two lines below is not clear, since we haven't written any train method for them)\n",
        "  graphsage_model.train()\n",
        "  link_predictor.train()\n",
        "\n",
        "  # Iterate over batches of training edges (positive edges)\n",
        "  # pos edge samples are contained in a Tensor of shape [2, batch_size]\n",
        "  # For sending batches of data, we will use pytorch's DataLoader method\n",
        "\n",
        "  for pos_samples in DataLoader(pos_train_edges, batch_size, shuffle=True):\n",
        "\n",
        "    # set the gradients to zeros\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # run graphsage forward pass\n",
        "    node_embeddings = graphsage_model(initial_node_embeddings, edge_index) # h_v in the theory above\n",
        "\n",
        "    # sample batch_size negative edges; size = [2, batch_size]\n",
        "    neg_samples = negative_sampling(edge_index,\n",
        "                                    num_nodes=initial_node_embeddings.shape[0],\n",
        "                                    num_neg_samples= len(pos_samples),\n",
        "                                    method = 'dense'\n",
        "                                    ) \n",
        "    \n",
        "    # run linkPredictor forward pass on positive edge embeddings\n",
        "    pos_preds = link_predictor(node_embeddings[pos_samples[:,0]],\n",
        "                               node_embeddings[pos_samples[:,1]])\n",
        "    \n",
        "    # run linkPredictor forward pass on negative edge embeddings\n",
        "    neg_preds = link_predictor(node_embeddings[neg_samples[0]],\n",
        "                               node_embeddings[neg_samples[1]])\n",
        "    \n",
        "    preds = torch.concat((pos_preds, neg_preds))\n",
        "    labels = torch.concat((torch.ones_like(pos_preds),\n",
        "                           torch.zeros_like(neg_preds)))\n",
        "    \n",
        "    loss = F.binary_cross_entropy(preds,labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    num_examples = len(pos_preds)\n",
        "    total_loss += loss.item() * num_examples\n",
        "    total_examples += num_examples\n",
        "\n",
        "    return total_loss/total_examples\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TIUysmV86kQ3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Parameter\n",
        "lr = 0.005 #@param {type: 'number'}\n",
        "batch_size = 65536 #@param {type:'number'}\n",
        "epochs = 50 #@param {type: 'number'}\n",
        "eval_steps = 5 #@param {type: 'number'}"
      ],
      "metadata": {
        "id": "PY1PMNMrUFqN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(list(graphsage_model.parameters()) + list(initial_node_embeddings.parameters())+list(link_predictor.parameters()),lr=lr)"
      ],
      "metadata": {
        "id": "uk1LM9B7VWye"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation (Task specific)"
      ],
      "metadata": {
        "id": "bQDYl24efNlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_valid_edges = valid_edges['edge'].to(device)\n",
        "neg_valid_edges = valid_edges['edge_neg'].to(device)\n",
        "pos_test_edges = test_edges['edge'].to(device)\n",
        "neg_test_edges = test_edges['edge_neg'].to(device)"
      ],
      "metadata": {
        "id": "4blpPxAIe85Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this link prediction, evaluation is done via a metric called 'Hits'. For definition, look at the original reference code. "
      ],
      "metadata": {
        "id": "WV6YBrc6f87G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.linkproppred import Evaluator\n",
        "\n",
        "evaluator = Evaluator(name=dataset_name)"
      ],
      "metadata": {
        "id": "cTFaRH49fX7H"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(graphsage_model, link_predictor, initial_node_embeddings, edge_index, \n",
        "         pos_valid_edges,neg_valid_edges, pos_test_edges, neg_test_edges, batch_size, evaluator):\n",
        "  \n",
        "  # run graphsage forward pass\n",
        "  final_node_embeddings = graphsage_model(initial_node_embeddings, edge_index)\n",
        "\n",
        "  # run link_predictor forward pass on positive validation edge embeddings\n",
        "  pos_valid_preds = []\n",
        "  \n",
        "  for pos_samples in DataLoader(pos_valid_edges, batch_size):\n",
        "    pos_preds = link_predictor(final_node_embeddings[pos_samples[:,0]],\n",
        "                               final_node_embeddings[pos_samples[:,1]])\n",
        "    \n",
        "    pos_valid_preds.append(pos_preds.squeeze()) # look up squeeze()\n",
        "\n",
        "  pos_valid_pred = torch.cat(pos_valid_preds, dim=0) # difference between torch.concat and torch.cat ?\n",
        "\n",
        "  # run link_predictor forward pass on negative validation edge embeddings\n",
        "  neg_valid_preds = []\n",
        "\n",
        "  for neg_samples in DataLoader(neg_valid_edges, batch_size):\n",
        "    neg_preds = link_predictor(final_node_embeddings[neg_samples[:,0]],\n",
        "                               final_node_embeddings[neg_samples[:,1]])\n",
        "    neg_valid_preds.append(neg_preds.squeeze()) # look up squeeze()\n",
        "  \n",
        "  neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
        "\n",
        "# run link_predictor forward pass on positive test edge embeddings\n",
        "  pos_test_preds = []\n",
        "  \n",
        "  for pos_samples in DataLoader(pos_test_edges, batch_size):\n",
        "    pos_preds = link_predictor(final_node_embeddings[pos_samples[:,0]],\n",
        "                               final_node_embeddings[pos_samples[:,1]])\n",
        "    \n",
        "    pos_test_preds.append(pos_preds.squeeze()) # look up squeeze()\n",
        "\n",
        "  pos_test_pred = torch.cat(pos_test_preds, dim=0) # difference between torch.concat and torch.cat ?\n",
        "\n",
        "  # run link_predictor forward pass on negative test edge embeddings\n",
        "  neg_test_preds = []\n",
        "\n",
        "  for neg_samples in DataLoader(neg_test_edges, batch_size):\n",
        "    neg_preds = link_predictor(final_node_embeddings[neg_samples[:,0]],\n",
        "                               final_node_embeddings[neg_samples[:,1]])\n",
        "    neg_test_preds.append(neg_preds.squeeze()) # look up squeeze()\n",
        "  \n",
        "  neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
        "\n",
        "\n",
        "  # Calculate Hits20\n",
        "  evaluator.K = 20\n",
        "  valid_hits = evaluator.eval({'y_pred_pos': pos_valid_pred, 'y_pred_neg': neg_valid_pred})\n",
        "  test_hits = evaluator.eval({'y_pred_pos': pos_test_pred, 'y_pred_neg': neg_test_pred})\n",
        "\n",
        "  return valid_hits, test_hits\n"
      ],
      "metadata": {
        "id": "-NhogsO3gTwt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the training and evaluation"
      ],
      "metadata": {
        "id": "XHgZDUPkkb68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_bar = trange(1,epochs+1, desc='Loss n/a')\n",
        "\n",
        "edge_index = ddi_graph.edge_index.to(device)\n",
        "pos_train_edges = train_edges['edge'].to(device)\n",
        "\n",
        "losses = []\n",
        "valid_hits_list = []\n",
        "test_hits_list = []\n",
        "\n",
        "for epoch in epochs_bar:\n",
        "  loss = train(graphsage_model, link_predictor, initial_node_embeddings.weight,edge_index, \n",
        "               pos_train_edges, optimizer, batch_size)\n",
        "  losses.append(loss)\n",
        "\n",
        "  epochs_bar.set_description(f'Loss {loss:0.4f}')\n",
        "\n",
        "  if epoch % eval_steps == 0:\n",
        "    valid_hits, test_hits = test(graphsage_model, link_predictor, initial_node_embeddings.weight,\n",
        "                                 edge_index,\n",
        "                                 pos_valid_edges, pos_test_edges,\n",
        "                                 neg_valid_edges,neg_test_edges,\n",
        "                                 batch_size, evaluator)\n",
        "    \n",
        "    print(f'\\nEpoch:{epoch},Validation Hits@20:{valid_hits[\"hits@20\"]:0.4f},Test Hits@20:{test_hits[\"hits@20\"]:0.04f}')\n",
        "\n",
        "    valid_hits_list.append(valid_hits[\"hits@20\"])\n",
        "    test_hits_list.append(test_hits[\"hits@20\"])\n",
        "  else:\n",
        "    valid_hits_list.append(valid_hits_list[-1] if valid_hits_list else 0)\n",
        "    test_hits_list.append(test_hits_list[-1] if test_hits_list else 0)\n",
        "\n",
        "plt.title(dataset.name + \": GraphSAGE\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.plot(losses, label = \"Training loss\")\n",
        "plt.plot(valid_hits_list, label = \"Validation Hits@20\")\n",
        "plt.plot(test_hits_list, label = \"Test Hits@20\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "kdJPiPKbketN",
        "outputId": "7c797f6b-18fa-4e46-b198-b82cc3af7f77"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.6890:  10%|█         | 5/50 [00:07<01:00,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:5,Validation Hits@20:0.0002,Test Hits@20:0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.6567:  20%|██        | 10/50 [00:11<00:48,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:10,Validation Hits@20:0.0002,Test Hits@20:0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.5888:  30%|███       | 15/50 [00:15<00:37,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:15,Validation Hits@20:0.0004,Test Hits@20:0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.5185:  40%|████      | 20/50 [00:20<00:34,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:20,Validation Hits@20:0.0001,Test Hits@20:0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.4665:  50%|█████     | 25/50 [00:24<00:25,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:25,Validation Hits@20:0.0002,Test Hits@20:0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.4423:  60%|██████    | 30/50 [00:28<00:21,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:30,Validation Hits@20:0.0003,Test Hits@20:0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.4238:  70%|███████   | 35/50 [00:33<00:17,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:35,Validation Hits@20:0.0002,Test Hits@20:0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.4105:  80%|████████  | 40/50 [00:37<00:10,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:40,Validation Hits@20:0.0005,Test Hits@20:0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.3863:  90%|█████████ | 45/50 [00:41<00:05,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:45,Validation Hits@20:0.0003,Test Hits@20:0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss 0.3720: 100%|██████████| 50/50 [00:45<00:00,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:50,Validation Hits@20:0.0003,Test Hits@20:0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z03bPueD0QNf",
        "outputId": "446a67ad-a8de-43c2-e55b-496a43dab206"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4039, 2424, 4039,  ...,  338,  835, 3554],\n",
              "        [2424, 4039,  225,  ...,  708, 3554,  835]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}